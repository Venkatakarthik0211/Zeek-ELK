{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71e7209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, string, nltk, boto3\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d6f4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# Check if NLTK resources are already downloaded\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords.zip')\n",
    "    nltk.data.find('corpora/wordnet.zip')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lem = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46b92c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text.lower())\n",
    "    tokens = re.split('\\W+', text)\n",
    "    tokens = [word for word in tokens if word not in stopwords and word]\n",
    "    tokens = [lem.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9186a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_percentage(text):\n",
    "    if len(text) == 0 or len(text.strip()) == 0:\n",
    "        return 0.0  # Return 0% if the text is empty or contains only whitespace\n",
    "    \n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count / (len(text) - text.count(' ')), 3) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53dd62f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing S3 Bucket, Which Contains CSV File\n",
    "bucket_name = 'mlnlp0608'\n",
    "file_key = 'SMSSpamCollection.tsv'\n",
    "\n",
    "# Create a boto3 client to interact with S3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Fetch the CSV file from S3\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "\n",
    "# Read the CSV content\n",
    "csv_content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "# Convert the CSV content to a pandas DataFrame with specific parameters\n",
    "df = pd.read_csv(StringIO(csv_content), sep=\"\\t\", header=None, names=['Person', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a7f91a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample_Training_Dataset.csv already exists in mlnlp0608. Clearing content and uploading new data...\n",
      "Sample_Training_Dataset.csv has been updated in mlnlp0608.\n"
     ]
    }
   ],
   "source": [
    "# Split the Training Data Set for NLP Modal\n",
    "# Adding a feature - punctuation percentage in the text\n",
    "df['punctuation_percentage'] = df['text'].apply(punctuation_percentage)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['text', 'punctuation_percentage']], df['Person'], test_size=0.2)\n",
    "        \n",
    "# Save the first 10 lines of training data to a CSV file\n",
    "# X_train.head(10).to_csv('training_data.csv', index=False)\n",
    "# Put the training dataset in the S3 Bucket\n",
    "\n",
    "# Uploading to Sample_Training_Dataset.csv\n",
    "FILE_NAME = 'Sample_Training_Dataset.csv'\n",
    "\n",
    "csv_content = X_train.head(10).to_csv(index=False)\n",
    "\n",
    "try:\n",
    "    # Check if the file exists\n",
    "    s3_client.head_object(Bucket=bucket_name, Key=FILE_NAME)\n",
    "    print(f'{FILE_NAME} already exists in {bucket_name}. Clearing content and uploading new data...')\n",
    "    \n",
    "    # Clear existing content by uploading new data\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=FILE_NAME, Body=csv_content)\n",
    "    print(f'{FILE_NAME} has been updated in {bucket_name}.')\n",
    "    \n",
    "except s3_client.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == '404':\n",
    "        # File does not exist, create it\n",
    "        print(f'{FILE_NAME} does not exist. Creating file...')\n",
    "        s3_client.put_object(Bucket=bucket_name, Key=FILE_NAME, Body=csv_content)\n",
    "        print(f'{FILE_NAME} has been created in {bucket_name}.')\n",
    "    else:\n",
    "        print(f'Error checking file existence: {e}')\n",
    "except (NoCredentialsError, PartialCredentialsError) as e:\n",
    "    print(f'Credentials error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a573575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a6e14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
